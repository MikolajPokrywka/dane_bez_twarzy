torch>=2.0.0
transformers>=4.36.0
peft>=0.7.0
bitsandbytes>=0.41.0
accelerate>=0.25.0
datasets>=2.16.0
scipy>=1.11.0
sentencepiece>=0.1.99
protobuf>=3.20.0

# Optional: vLLM for faster inference (10-20x speedup)
vllm>=0.2.7
sacrebleu

